Determine American Sign Language letter based on the image of a human hand.

![MNIST-sign-language](https://miro.medium.com/v2/resize:fit:665/1*MLudTwKUYiCYQE0cV7p6aQ.png)

## About
This project uses machine learning to recognize American Sign Language (ASL) letters from images of human hands.

## Contributors

[![Sithumini](https://avatars.githubusercontent.com/u/sithuminikaushalya?v=4)](https://github.com/sithuminikaushalya)  
**Sithumini** - Contributor

[![Aralugaswaththa CVCRP](https://avatars.githubusercontent.com/u/CPrasa?v=4)](https://github.com/CPrasa)  
**Sivasothy Tharsi** - Developer

[![Madhunicka](https://avatars.githubusercontent.com/u/Madhunicka?v=4)](https://github.com/Madhunicka)  
**Mathanamohan Madhunicka** - Contributor

[![Tharsi](https://avatars.githubusercontent.com/u/Sivasothy-Tharsi?v=4)](https://github.com/Sivasothy-Tharsi)  
**Sivasothy Tharsi** - Developer

Aralugaswaththa CVCRP

## Student Names and Registration Numbers

- Amarasinghe WLSK (EG_2020_3818)
- Aralugaswaththa CVCRP (EG_2020_3827)
- Madunica M (EG_2020_4051)
- Tharsi S (EG_2020_4232)

## Hand Gesture Recognition using Sign Language MNIST Dataset

In this AI project, we aim to develop a recognition system to identify hand gestures from the Sign Language MNIST dataset. This dataset comprises various hand gestures representing letters from the American Sign Language (ASL) alphabet, excluding 'J' and 'Z' due to their requirement for motion. By leveraging this dataset, we seek to build a robust visual recognition algorithm that can aid in the communication for the deaf and hard-of-hearing community.

### Dataset Overview

The dataset follows the structure of the original MNIST dataset, a widely recognized benchmark for image-based machine learning tasks. Each image in the dataset is a 28x28 pixel grayscale representation of a hand gesture, with pixel values ranging from 0 to 255. The dataset is organized in a CSV format where each row represents an image, starting with a label followed by 784 pixel values.

- **Training Data**: 27,455 images
- **Test Data**: 7,172 images
- **Classes**: 24 (letters A-Z, excluding 'J' and 'Z')

### Data Generation and Preprocessing

The images were generated by extending a smaller set of color images through an image processing pipeline using ImageMagick. The pipeline involved several steps:
- Cropping to focus on the hand region
- Converting to grayscale
- Resizing to 28x28 pixels
- Augmenting the data by applying various filters (Mitchell, Robidoux, Catrom, Spline, Hermite), adding random pixelation, adjusting brightness/contrast, and slight rotations.

This preprocessing aimed to create a diverse and challenging dataset that better simulates real-world conditions and provides a robust training set for machine learning models.

### Project Goals

1. **Exploratory Data Analysis (EDA)**: Understanding the distribution of the data, visualizing the images, and identifying any patterns or anomalies.
2. **Data Preprocessing**: Cleaning and preparing the data for model training, including normalization and augmentation.
3. **Model Development**: Implementing various machine learning models, such as Convolutional Neural Networks (CNNs), to accurately recognize hand gestures.
4. **Evaluation and Optimization**: Assessing model performance using metrics like accuracy, precision, and recall, and fine-tuning the models for better results.

### Significance

Developing an accurate hand gesture recognition system has significant practical applications. It can enhance communication for the deaf and hard-of-hearing community, provide new benchmarks for computer vision research, and potentially be implemented in affordable, portable devices like Raspberry Pi for real-time translation.

By the end of this project, we aim to deliver a high-performing gesture recognition system that can serve as a foundation for further developments in assistive technologies and advanced computer vision applications.
